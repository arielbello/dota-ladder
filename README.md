Dota Ladder Statistics
======================

A simple project to create a website displaying statistics from http://www.dota2.com/leaderboards.

How it should work
----------------------
This project bundles both the front-end and back-end. 

The front-end is a static webpage, but its content should be created and updated by its counterpart.

The back-end consists of Python scripts to scrap data using Selenium Webdriver (Firefox, gecko), 
then update the website's server with files reflecting a new data state.
This approach turns the front-end very light and simple, while the heavy-lifting,
complexity and setup are left to the back-end.

Project file structure
----------------------
- **software-engineering**: Files regarding my thought proccess to develop this project. 
Things like listing tasks and figuring out project components. Nothing to do with traditional software engineering.
- **scraping**: Files related to web scraping. The back-end part of this project. Done with Python using Selenium.
- **webpage**: Website files. The front-end part. The website part doesn't work only with these files, they need
the data generated by back-end (scrapers).

How to
---------------------
To scrape the data:
I accomplished this using a Google Cloud Compute VM. I set up a linux VM like so:
(It should be possible to run serverless functions like Cloud Run)
1- Install Python
2- Install Selenium (pip install selenium)
3- Install Firefox
4- Download Firefox webdriver (https://github.com/mozilla/geckodriver/releases)
5- Add the webdriver to PATH. TIP: move to `/opt/Webdriver/bin/` and add this folder to PATH
6- Run scripts

To display a website:
I accomplieshed this using Google Cloud Storage. There are many other ways to do this, including use the same VM used for scraping.
1- Serve the files in the webpage folder
Ok, but still you don't have the data to display in the chart, so:

Put everything together:
1- Upload the generated files from the scraping service (first list) to your webserver
2- Create a scheduler (Google Cloud Scheduler) or a cron job (Linux's `crontab`) to
periodically run the data mining and data uploading scripts, so your website is up to date!

License
---------------------
This project is available under the [MIT license](https://opensource.org/licenses/MIT).